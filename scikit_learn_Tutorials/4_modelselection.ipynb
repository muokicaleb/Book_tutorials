{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection: choosing estimators and their parameters\n",
    "\n",
    "### Score and cross-validated scores.\n",
    "\n",
    "As we have seen, every estimator exposes a `score` method that can judge the quality of the fit (or the prediction) on new data. Bigger is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, svm\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in folds that we use for training and testing. This is called a KFold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9348914858096828, 0.9565943238731218, 0.9398998330550918]\n"
     ]
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> X_folds = np.array_split(X_digits, 3)\n",
    ">>> y_folds = np.array_split(y_digits, 3)\n",
    ">>> scores = list()\n",
    ">>> for k in range(3):\n",
    "...     # We use 'list' to copy, in order to 'pop' later on\n",
    "...     X_train = list(X_folds)\n",
    "...     X_test = X_train.pop(k)\n",
    "...     X_train = np.concatenate(X_train)\n",
    "...     y_train = list(y_folds)\n",
    "...     y_test = y_train.pop(k)\n",
    "...     y_train = np.concatenate(y_train)\n",
    "...     scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
    ">>> print(scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation generators\n",
    "\n",
    "Scikit-learn has a collection of classes which can be used to generate lists of train/test indices for popular cross-validation strategies.\n",
    "\n",
    "They expose a `split` method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.\n",
    "\n",
    "This example shows an example usage of the `split` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5 6 7 8 9] | test: [0 1]\n",
      "Train: [0 1 4 5 6 7 8 9] | test: [2 3]\n",
      "Train: [0 1 2 3 6 7 8 9] | test: [4 5]\n",
      "Train: [0 1 2 3 4 5 8 9] | test: [6 7]\n",
      "Train: [0 1 2 3 4 5 6 7] | test: [8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"c\", \"c\", \"c\"]\n",
    "k_fold = KFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "     print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation can then be performed easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9638888888888889,\n",
       " 0.9222222222222223,\n",
       " 0.9637883008356546,\n",
       " 0.9637883008356546,\n",
       " 0.9303621169916435]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])\n",
    " for train, test in k_fold.split(X_digits)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation score can be directly calculated using the `cross_val_score` helper. Given an estimator, the `cross-validation` object and the input dataset, the cross_val_score splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.\n",
    "By default the estimator’s score method is used to compute the individual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96388889, 0.92222222, 0.9637883 , 0.9637883 , 0.93036212])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_jobs=-1 means that the computation will be dispatched on all the CPUs of the computer.\n",
    "\n",
    "### Grid-search and cross-validated estimators\n",
    "\n",
    "#### Grid-search\n",
    "\n",
    "scikit-learn provides an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muoki/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),\n",
    "                    n_jobs=-1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])        \n",
    "\n",
    "clf.best_score_                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9435382685069009"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.C                            \n",
    "\n",
    "# Prediction performance on test set is not as good as on train set\n",
    "clf.score(X_digits[1000:], y_digits[1000:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the GridSearchCV uses a 3-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validated estimators\n",
    "\n",
    "Cross-validation to set a parameter can be done more efficiently on an algorithm-by-algorithm basis. This is why, for certain estimators, scikit-learn exposes Cross-validation: evaluating estimator performance estimators that set their parameter automatically by cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=3, eps=0.001, fit_intercept=True,\n",
       "        max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,\n",
       "        positive=False, precompute='auto', random_state=None,\n",
       "        selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "lasso = linear_model.LassoCV(cv=3)\n",
    "diabetes = datasets.load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "lasso.fit(X_diabetes, y_diabetes)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012291895087486173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The estimator chose automatically its lambda:\n",
    "lasso.alpha_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These estimators are called similarly to their counterparts, with ‘CV’ appended to their name."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
